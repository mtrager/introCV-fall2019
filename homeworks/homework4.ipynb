{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due date:** December 17th 2019\n",
    "\n",
    "The goal of the assignment is to train a simple neural network on MNIST data.\n",
    "\n",
    "*Note:* This notebook requires Python 3.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Read and understand the provided code, which is a basic implementation of a neural network as a modular architecture.\n",
    "* Download the [MNIST data](http://deeplearning.net/data/mnist/mnist.pkl.gz) (~15Mo). It consists of 28x28 images (loaded as a 784 vector) and the associated labels for training, validation and test sets. For this homework, you will only use the training and validation sets. \n",
    "* Write a simple loop to train 50 iterations of the implemented MLP (multi-layer perceptron) with a learning rate 0.001 and batches of size 16. Plot the training and validation losses throughout the training process (you don't have to test your network at every iteration, you can do it for example every 10 iterations).\n",
    "* Evaluate the accuracy of your trained model on the training and validation data. Check the predictions on random validation images.\n",
    "* Try changing learning rate and batch size and see if you can improve your results.\n",
    "* How many parameters does the network have? Implement and experiment with some variations of the architecture, for example:\n",
    "    * Implement and use the cross-entropy loss instead of L2 loss.\n",
    "    * Add a parameter to vary the size of the intermediate layer.\n",
    "    * Use a MLP with 3 layers and parameters for the sizes of the two intermediate layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MLP implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self.gradInput=None \n",
    "        self.output=None\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "        Should be overriden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *input):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "        Should be overriden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class LeastSquareCriterion(Module):\n",
    "    \"\"\"\n",
    "    This implementation of the least square loss assumes that the data comes as a 2 dimensional array\n",
    "    of size (batch_size,num_classes) and the labels as a vector of size (num_classes) \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeastSquareCriterion, self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "    \n",
    "    def forward(self, x,labels):\n",
    "        target=np.zeros([x.shape[0],self.num_classes])\n",
    "        for i in range(x.shape[0]):\n",
    "            target[i,labels[i]]=1\n",
    "        self.output = np.sum((target-x)**2,axis=0)\n",
    "        return np.sum(self.output)\n",
    "    \n",
    "    def backward(self, x, labels):\n",
    "        self.gradInput=x\n",
    "        for i in range(x.shape[0]):\n",
    "            self.gradInput[i,labels[i]]=x[i,labels[i]]-1\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    The input is supposed to have two dimensions (batchSize,in_feature)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = math.sqrt(1. / (out_features* in_features))*np.random.randn(out_features, in_features)\n",
    "        self.bias = np.zeros(out_features)\n",
    "        self.gradWeight=None\n",
    "        self.gradBias=None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.output= np.dot(x,self.weight.transpose())+np.repeat(self.bias.reshape([1,-1]),x.shape[0], axis=0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        self.gradInput=np.dot(gradOutput,self.weight)\n",
    "        self.gradWeight=np.dot(gradOutput.transpose(),x)\n",
    "        self.gradBias=np.sum(gradOutput, axis=0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def gradientStep(self,lr):\n",
    "        self.weight=self.weight-lr*self.gradWeight\n",
    "        self.bias=self.bias-lr*self.gradBias\n",
    "        \n",
    "\n",
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self, bias=True):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.output=x.clip(0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        self.gradInput=(x>0)*gradOutput\n",
    "        return self.gradInput\n",
    "    \n",
    "        \n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = Linear(784, 64)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, gradient):\n",
    "        gradient = self.fc2.backward(self.relu1.output,gradient)\n",
    "        gradient = self.relu1.backward(self.fc1.output,gradient)\n",
    "        gradient = self.fc1.backward(x,gradient)\n",
    "        return gradient\n",
    "    \n",
    "    def gradientStep(self,lr):\n",
    "        self.fc2.gradientStep(lr)\n",
    "        self.fc1.gradientStep(lr)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load and visualize the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '' # where you saved the dataset\n",
    "f = gzip.open(path + 'mnist.pkl.gz', 'rb') \n",
    "train_set, val_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "train_data=train_set[0]\n",
    "train_labels=train_set[1]\n",
    "val_data=val_set[0]\n",
    "val_labels=val_set[1]\n",
    "N_train=train_data.shape[0]\n",
    "N_val=val_data.shape[0]\n",
    "# check that the data makes sense\n",
    "plt.imshow(train_data[0,:].reshape(28,28))\n",
    "print(train_labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
